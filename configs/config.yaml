model_paths:
  reward_model: '../../results/reward/checkpoint-4221'
  reward_tokenizer: '../../results/reward/checkpoint-4221'
  sft_model: 'lvwerra/gpt2-imdb'
  output_model: '../../results/warp_model'

training_params:
  iterations: 2  #I
  training_steps: 150 #T
  rl_runs: 2 #M
  ema_update_rate: 0.01 #mu
  interpolation_factor: 0.5 #nu
  kl_coefficient: 0.01 #beta
  batch_size: 512 
  learning_rate: 1e-5

dataset:
  num_positive_samples: 100
  num_negative_samples: 100
  max_length: 512
  max_prompts: 128
  test_max_prompts: 100
  min_tokens: 5
  max_tokens: 18

reward:
  output_dir: '../../results/reward'
  max_length: 512
  num_train_epochs: 3
  per_device_train_batch_size: 16

device: 'cuda'  
max_length: 25
